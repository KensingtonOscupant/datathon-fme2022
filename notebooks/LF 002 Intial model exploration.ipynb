{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import pipeline\n",
    "from sklearn import model_selection\n",
    "from sklearn import linear_model # 63%\n",
    "from sklearn import tree # 76%\n",
    "from sklearn import svm # Linear 65%\n",
    "from sklearn import naive_bayes # 63%\n",
    "from sklearn import neighbors # 73% first try\n",
    "from sklearn import neural_network\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from haversine import haversine, Unit\n",
    "import xgboost\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(context='paper', font_scale=1.15)\n",
    "\n",
    "data_dir = '../data/datathon_SC_ACN_22/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_data = pd.read_csv(data_dir + 'orders.csv', delimiter=';', index_col='order_id')\n",
    "cities_data = pd.read_csv(data_dir + 'cities_data.csv', delimiter=';')\n",
    "product_data = pd.read_csv(data_dir + 'product_attributes.csv', delimiter=',', index_col='product_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "- Lat long for all three locations\n",
    "- Distance on water\n",
    "- Distance on land\n",
    "- Distance on land **log**\n",
    "- Units\n",
    "- Units **log**\n",
    "- Third party **ONH**\n",
    "- Customs **ONH**\n",
    "- Material handling **ONH**\n",
    "- Weight\n",
    "- Weight **log**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace ATHENAS with Athens and BCN with Barcelona in the origin_port column\n",
    "orders_data['origin_port'] = orders_data['origin_port'].replace('ATHENAS', 'Athens')\n",
    "orders_data['origin_port'] = orders_data['origin_port'].replace('BCN', 'Barcelona')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_data1 = cities_data[['city_from_name', 'city_from_coord']]\n",
    "cities_data2 = cities_data[['city_to_name', 'city_to_coord']]\n",
    "cities_data1 = cities_data1.rename(columns={'city_from_name': 'city_name', 'city_from_coord': 'city_coord'})\n",
    "cities_data2 = cities_data2.rename(columns={'city_to_name': 'city_name', 'city_to_coord': 'city_coord'})\n",
    "\n",
    "cities_data = pd.concat([cities_data1, cities_data2]).drop_duplicates().set_index('city_name')\n",
    "\n",
    "# Transform lat long tuple as string to two columns\n",
    "cities_data['lat'] = cities_data['city_coord'].apply(lambda x: float(x[1:-1].split(',')[0]))\n",
    "cities_data['long'] = cities_data['city_coord'].apply(lambda x: float(x[1:-1].split(',')[1]))\n",
    "cities_data = cities_data.drop(columns=['city_coord'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only for those extracted columns\n",
    "city_lat_longs = cities_data[['lat', 'long']].drop_duplicates()\n",
    "\n",
    "# Add to order df\n",
    "orders_data = orders_data.join(city_lat_longs.rename(columns={'lat': 'start_lat', 'long': 'start_long'}), on='origin_port')\n",
    "orders_data = orders_data.join(city_lat_longs.rename(columns={'lat': 'hub_lat', 'long': 'hub_long'}), on='logistic_hub')\n",
    "orders_data = orders_data.join(city_lat_longs.rename(columns={'lat': 'end_lat', 'long': 'end_long'}), on='customer')\n",
    "\n",
    "orders_data.loc[orders_data['logistic_hub'].isna(), 'hub_lat'] = orders_data.loc[orders_data['logistic_hub'].isna(), 'start_lat']\n",
    "orders_data.loc[orders_data['logistic_hub'].isna(), 'hub_long'] = orders_data.loc[orders_data['logistic_hub'].isna(), 'start_long']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distances\n",
    "orders_data['distance_on_water'] = orders_data.apply(lambda x: haversine((x['start_lat'], x['start_long']), (x['hub_lat'], x['hub_long'])), axis=1)\n",
    "orders_data['distance_on_land'] = orders_data.apply(lambda x: haversine((x['hub_lat'], x['hub_long']), (x['end_lat'], x['end_long'])), axis=1)\n",
    "orders_data.loc[orders_data['logistic_hub'].isna(), 'distance_on_land'] = orders_data.loc[orders_data['logistic_hub'].isna()].apply(lambda x: haversine((x['start_lat'], x['start_long']), (x['end_lat'], x['end_long'])), axis=1)\n",
    "\n",
    "# Log version of distance on land\n",
    "orders_data['distance_on_land_log'] = orders_data['distance_on_land'].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log version of units of order\n",
    "orders_data['units_log'] = orders_data['units'].apply(lambda x: np.log(x))\n",
    "\n",
    "orders_data = orders_data.join(product_data, on='product_id')\n",
    "\n",
    "# Log version of product weight\n",
    "orders_data['weight_log'] = orders_data['weight'].apply(lambda x: np.log(x))\n",
    "\n",
    "# Convert material handling class as int to string class\n",
    "orders_data['material_handling'] = orders_data['material_handling'].apply(lambda x: \"c\" + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle no hub orders\n",
    "# if hub == nan then replace sea_distance with 0\n",
    "orders_data['distance_on_water'] = orders_data['distance_on_water'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_data.loc[orders_data['distance_on_land'] == 0, 'distance_on_land_log'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nas, which in this case should only be products without information\n",
    "orders_data = orders_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_target = orders_data['late_order']\n",
    "orders_features = orders_data.drop(columns=['origin_port', 'logistic_hub', 'customer', 'late_order', 'product_id'])\n",
    "\n",
    "orders_features = pd.get_dummies(orders_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['units', 'start_lat', 'start_long', 'hub_lat', 'hub_long', 'end_lat',\n",
       "       'end_long', 'distance_on_water', 'distance_on_land',\n",
       "       'distance_on_land_log', 'units_log', 'weight', 'weight_log',\n",
       "       '3pl_v_001', '3pl_v_002', '3pl_v_003', '3pl_v_004',\n",
       "       'customs_procedures_CRF', 'customs_procedures_DTD',\n",
       "       'customs_procedures_DTP', 'material_handling_c0.0',\n",
       "       'material_handling_c1.0', 'material_handling_c2.0',\n",
       "       'material_handling_c3.0', 'material_handling_c4.0',\n",
       "       'material_handling_c5.0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_features_not_delayed = orders_features.loc[orders_data['late_order'] == False].sample(frac=0.4)\n",
    "orders_features_delayed = orders_features.loc[orders_data['late_order'] == True]\n",
    "\n",
    "#Combine\n",
    "order_features_balanced = pd.concat([orders_features_not_delayed, orders_features_delayed]).sample(frac=1)\n",
    "order_target_balanced = order_features_balanced.join(orders_target)['late_order']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_target_int = orders_target * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(orders_features, orders_target_int, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8194235699761294"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ada = ensemble.AdaBoostClassifier(n_estimators=100, learning_rate=1.5)\n",
    "model_rf = ensemble.RandomForestClassifier(max_depth=20)\n",
    "model_xgb = xgboost.XGBClassifier(\n",
    "    scale_pos_weight=0.4,\n",
    "    n_estimators=300,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.15,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=0.8,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "model_nnb = neighbors.KNeighborsClassifier()\n",
    "model_nn = neural_network.MLPClassifier(hidden_layer_sizes=(10,30,10), learning_rate='adaptive', max_iter=1000)\n",
    "\n",
    "voting_classifier = ensemble.VotingClassifier(estimators=[\n",
    "    ('ada', model_ada),\n",
    "    ('rf', model_rf),\n",
    "    ('xgb', model_xgb),\n",
    "    ('nnb', model_nnb),\n",
    "    ('nn', model_nn)], voting='hard')\n",
    "\n",
    "pipe = pipeline.make_pipeline(preprocessing.StandardScaler(), voting_classifier)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16718, 1819, 3560, 525)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = metrics.confusion_matrix(y_test, pipe.predict(X_test)).ravel()\n",
    "tn, tp, fn, fp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('datathon-fme2022')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a5f1cad13e83ff8f38af6883a908047d62aca5621397592ff06c4814667afdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
